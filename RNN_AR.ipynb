{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import StringIO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Holt</td>\n",
       "      <td>Good evening from Hofstra University in Hempst...</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Audience</td>\n",
       "      <td>(APPLAUSE)</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>How are you, Donald?</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Audience</td>\n",
       "      <td>(APPLAUSE)</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Holt</td>\n",
       "      <td>Good luck to you.</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Line   Speaker                                               Text     Date\n",
       "0     1      Holt  Good evening from Hofstra University in Hempst...  9/26/16\n",
       "1     2  Audience                                         (APPLAUSE)  9/26/16\n",
       "2     3   Clinton                               How are you, Donald?  9/26/16\n",
       "3     4  Audience                                         (APPLAUSE)  9/26/16\n",
       "4     5      Holt                                  Good luck to you.  9/26/16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('debate.csv')\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = urllib.urlopen('http://www.presidency.ucsb.edu/ws/index.php?pid=119181').read()\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "#speech = soup.find_all('span')[6]\n",
    "speech = soup.find_all('p')\n",
    "for sentence in speech:\n",
    "    sentence = str(sentence)[3:-4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>Before I begin with my remarks, I do want to say how proud I am of our brave first responders working to keep us safe after the attacks of the last weekend in New York, New Jersey and Minnesota. There are now reports of a suspect in custody, but we must remain vigilant. This is a fast-moving situation and a sobering reminder that we need steady leadership in a dangerous world.</p>,\n",
       " <p>I'm here to talk about a number of the issues that are part of this election but really, much more than that, they are part of our future \\u2013 the kind of country we want to have, the kind of people we want to be and, particularly, what kind of opportunities we should be providing for the young people of America. I have a proud Owl on my staff, Jamira Burley. A Philadelphia native who became an activist working to end the epidemic of gun violence right here in Philadelphia. She loves Temple and we love her. And I also want to thank Lauren for that introduction.</p>,\n",
       " <p>Jamira and Lauren are two examples of why I do have so much faith in our future. Your generation is the most inclusive, progressive and entrepreneurial that we've ever seen. And as you heard, when Lauren was in college, she saw challenges facing students of color, but there was no NAACP chapter to support them and promote diversity and inclusion on campus \\u2013 so she started one. And Lauren remains committed and engaged, working with an organization called Generation Progress, because she understands that active citizenship is a lifelong job and the call to service never fades.</p>,\n",
       " <p>Now, I know that with so much negativity out there, it is really easy to get cynical \\u2013 especially about our politics. I remember wrestling with that challenge when I was a student during the Vietnam War. It can be tempting to think that no one will tell you the truth and nothing's ever going to change. But you're here today because you refuse to accept cynicism. You know that the next 50 days will shape the next 50 years. And you see how much needs fixing in our country, from the soaring cost of college to the scourge of systemic racism to the threat from climate change \\u2013 but you also know the only way we can meet those challenges is if we meet them together.</p>,\n",
       " <p>You're here today because you believe we can do just that. You want something to vote for, not just against. Optimism, not resentment. Answers, not anger. Ideas, not insults. Bridges, not walls. You're also here because you know this election isn't a reality TV show. It shouldn't be about birth certificates or name-calling or stunts to get onto cable news.</p>,\n",
       " <p>This election comes down to a choice between two very different visions for America. I believe it's wrong to tear each down. We should be lifting each other up. It's wrong to let income inequality get even worse. We have to make the economy work for everyone, not just those at the top. And it's wrong to put a loose cannon in charge who could start another war. We should work with our allies to keep us safe.</p>,\n",
       " <p>It comes down to this: Are we going to pit Americans against each other and deepen the divides in this country, or are we going to be, as I know we can, stronger together?</p>,\n",
       " <p>I know what I believe, and I'm going to close my campaign the same way I started my career \\u2013 fighting for kids and young people and families. That has been the cause of my life. And it will be the passion of my presidency, and I hope you'll join me. We can't get distracted when the media or my opponent turns this election into a circus. My husband has a saying about that. He calls it majoring in the minors \\u2013 getting so wrapped up in stuff that doesn't matter, you forget what's really important to your future and to the future of this country.</p>,\n",
       " <p>Take the challenges facing young Americans today. First of all, if you're willing to work hard, you should be able to find a good job that pays well and lets you do what you love and make your mark in the world. But that's been out of reach for too many young people \\u2013 trying to find your footing in the wake of the worst economic crisis since the Great Depression. That's why Tim Kaine and I have a plan to work with both parties and make a historic investment in good new jobs. We can create millions of jobs and make life a lot better by doing things like connecting every household to broadband by 2020; installing half a billion solar panels; building a cleaner, more resilient electric grid with enough renewable energy to power every home in the country.</p>,\n",
       " <p>Next, getting an education should give you a boost, not hold you back. But as you know better than most, tuition keeps going through the roof and debt keeps piling up. I understand that Temple was founded to democratize, diversify and widen the reach of higher education. That is still a vital goal.</p>,\n",
       " <p>So I worked with Bernie Sanders on a plan. We came up with a plan that makes public college tuition-free for working families and debt-free for everyone. And if you already have debt, we will help you refinance it and pay it back as a percentage of your income so you're never on the hook for more than you can afford. You can actually see how much you and your family can save under our plan by looking at the College Calculator at hillaryclinton.com.</p>,\n",
       " <p>And here's something we don't talk about enough: a four-year degree should not be the only path to a good job in America. People should be able to learn a skill, practice a trade, and making a good living because of that. So we're offering new tax credits to encourage companies to offer paid apprenticeships that let you earn while you learn and do more to dignify skills across the board \\u2013 for welders, machinists, health technicians, coders, and so many other fields.</p>,\n",
       " <p>Another challenge I hear about all the time is from new parents about how hard it is to balance the demands of work and family in today's economy. Families look different today than they did decades ago, I think we can all agree. Most need two incomes just to get by. And many people now change jobs frequently and have wildly unpredictable schedules, or they have to cobble together part-time work, all without the basic supports available to parents in nearly every other advanced country.</p>,\n",
       " <p>That's why Tim Kaine and I have a plan to help working families with quality, affordable childcare, preschool and paid family leave. We fundamentally believe \\u2013 the more we can strengthen families, the stronger we will be as a nation.</p>,\n",
       " <p>Everywhere I go, young people also share their concerns about the divisiveness and discrimination we see in America today. You aren't \\u2013 and you shouldn't be \\u2013 satisfied with the progress we've made. You should keep wanting to right wrongs and fight for justice and dignify for all. We see, as Lauren said, too many young black men and women made to feel like their lives are disposable; too many immigrants living in fear of deportation; too many young LGBT Americans bullied; too many young women and men sexually assaulted on campus or in the military or at home. And more than previous generations, you understand that all these challenges are intersecting, and we must take them on together.</p>,\n",
       " <p>But you also see a Republican nominee for president who incites hatred and violence like we've never seen before in any campaign. Hate speech being normalized. The dog whistles are out in the open. And yet, despite this, I remain convinced America's best days are ahead of us. In large part, that's because of the inspiring young people I meet every day.</p>,\n",
       " <p>I'm inspired by Astrid. I met her in Las Vegas last summer. She was brought to this country from Mexico at the age of four with nothing but a doll, a cross, and the dress she was wearing. Now she's in her twenties and advocating for the rights of undocumented Americans and comprehensive immigration reform. We should all join her in this.</p>,\n",
       " <p>I'm inspired by Mikey, who I met in New York. Mikey spent six months in prison for a low-level drug offense. After he got out, Mikey discovered just how hard it is for people who've done their time to find good jobs and opportunities. But he persisted, and he managed to start his own ice cream shop in New York \\u2013 and I can recommend, it's delicious. We have to do more to help others get that second chance, including by banning the box and reforming our criminal justice system.</p>,\n",
       " <p>I'm inspired by Erica, one of the bravest young women I've ever met. Her mother, Dawn, was the principal of Sandy Hook School who died trying to protect her students. Erica was devastated. But then, she made it her mission to advocate for commonsense gun safety reforms. It's been painful for her \\u2013 a lot of hate has come her way and the gun lobby is so powerful \\u2013 but Erica won't give up. As she said, 'What if everyone who faced tough odds said, it's hard, so I'm going to walk away? That's not the type of world I want to live in.'</p>,\n",
       " <p>That's the spirit that makes this country great \\u2013 we might get knocked down, but we get right back up again. And we refuse to quit, no matter what. And that's the spirit we need in this election too.</p>,\n",
       " <p>Now, I know that with Washington paralyzed by big money and partisanship, the gap between the change we want and the progress that politics should deliver can look like a chasm. I also know that even if you're totally opposed to Donald Trump, you may still have some questions about me. I get that. And I want to do my best to answer those questions.</p>,\n",
       " <p>When it comes to public service, the 'service' part has always easier for me than the 'public' part. I will never be the showman my opponent is \\u2013 and you know what, that's okay with me. And it's also true I do spend a lot of time on the details of policy, like the precise interest rate on your student loans, right down to the decimal. But that's because it's not a detail for you. It's a big deal. And it should be a big deal to your president.</p>,\n",
       " <p>So here's what I ask any voter who is still undecided: Give us both a fair hearing. Hold us accountable for our ideas, both of us. I can't promise you'll agree with me all the time, but I can promise you this: No one will work harder to make your life better. I will never stop, no matter how tough it gets. In fact, you can read about what Tim and I want to do. We're not keeping it a secret. We've got a book called 'Stronger Together.'</p>,\n",
       " <p>But let me tell you a little bit about the values that drive me and my vision for the future because you deserve that from anyone from anyone running for president. I want to share with you the stories of three women who at pivotal moments changed my life and set me on a course of social justice, activism, and public service.</p>,\n",
       " <p>The first woman is my mother. Her name was Dorothy, and she was abandoned by her parents as a young girl. She ended up out on her own at 14, working as a housemaid. When I learned about this many years later, I asked how she managed to grow up into a warm, loving person and not become bitter and broken. And here's what she said, one word: 'kindness.' She was saved by the kindness of others. Like the teacher who saw she had nothing to eat at lunch and brought extra food to share. The lesson she passed on to me was simple but powerful: No one gets through life alone. We have to look out for each other and lift each other up. She made sure I learned the words of one of the creeds of our Methodist faith: Do all the good you can for all the people you can in all the ways you can as long as ever you can.</p>,\n",
       " <p>That mission guides me still today. When I stumble, it helps pick me up, because there's always more good to do and more people to help if we keep our eyes open, especially kids.</p>,\n",
       " <p>When I met a terrified little girl in Nevada who burst into tears because she worried her parents would be deported, it hit me right in the gut. I knew how hard-working her parents were. I knew the sacrifices they were making so that she could have a better life. When I met a little boy in Flint, Michigan who got sick from drinking water poisoned with lead, it just made me so angry and determined to work even harder. Every one of our children deserves the chance to share in the promise of America.</p>,\n",
       " <p>The second woman I want to tell you about is Marian Wright Edelman. She was a lawyer for the NAACP in Mississippi, first African American woman to pass the Mississippi bar. She was an ally of Dr. King and Robert Kennedy, and the founder of the Children's Defense Fund, an altogether remarkable person. One day during my first semester in law school, I saw a flyer \\u2013 we used to have those, flyers on a campus bulletin board. And Marian was coming to give a lecture. I made sure to be there, and what I heard was captivating. Marian talked about creating a Head Start program in Mississippi and using her legal education to make life better for poor children and families. Something just clicked in my brain. I began to see how I could translate the commitment to helping others I'd learned from my mother, my church, into real social change. I went up to her and I said, 'Could I work for you this summer?' She said, 'Sure, but I can't pay you.' I said, 'Well, I am paying my way through law school, so I have to get paid.' She said, 'Well, if you can figure out how to get paid, you can have a job.' So I figured out how to get a grant to get paid and went to work for her.</p>,\n",
       " <p>After graduation, I could have followed my classmates to a high-powered law firm, but I went to work for Marian at the Children's Defense Fund instead. She sent me door to door in New Bedford, Massachusetts on behalf of children with disabilities who were denied the chance to go to school back then. And I remember meeting a young girl in a wheelchair on the small back porch of her house. She told me how badly she wanted an education, but it just didn't seem possible. My heart went out to her and I wanted to help. But it became clear to me that simply caring is not enough. That wouldn't force the public school to build more wheelchair ramps or put more resources into special education. I learned that to drive real progress, you have to change both hearts and laws. So we gathered evidence. We built a coalition. And our work helped convince Congress to ensure access to education for all students with disabilities. And that experience turned me into a lifelong advocate for children and families.</p>,\n",
       " <p>I went to South Carolina to investigate the plight of 12- and 13-year-old boys imprisoned alongside grown men who had committed serious felonies. In Alabama I helped expose the racism of segregated academies. In Arkansas I ran a legal aid clinic that provided representation to poor families and prison inmates who could not afford it. When Bill was elected President, a lot of people were surprised, and even threatened, by the idea of an activist as First Lady. But I wasn't about to quit then, either. I fought for universal health care, and ended up working with Republicans and Democrats in Congress to create the Children's Health Insurance Program, which covers eight million kids today. </p>,\n",
       " <p>The third woman who changed my life was named Sophia, the 17-year-old captain of a high school basketball team in New York City. It was the late 1990s, and Democrats in New York were urging me to run for the Senate, and I kept telling them no. After all, no First Lady had ever done anything like that. I myself hadn't run for anything since student council. I'd always been an advocate, not a politician. But then one day I visited that school in New York for an event with young women athletes with Billie Jean King. Hanging above our heads was a big banner that said, 'Dare to compete.' Before my speech, Sophia introduced me. She was tall, and as we shook hands, she bent over and whispered in my ear, 'Dare to compete, Mrs. Clinton. Dare to compete.' Once again, something just clicked. For years I had been telling young women to step up. Participate. Go for what you believe in. Could it be I was afraid to do something I had urge so many others to do?</p>,\n",
       " <p>Well, it was a difficult transition, becoming a candidate for the first time back in that New York Senate race. Even all these years later, I confess I don't enjoy doing some of the things that come naturally to most politicians, like talking about myself. But I took that leap then for the same reason I'm running now, to even the odds for those who've got the odds stacked against them, especially children and families. And I've learned that in a democracy, if you want to help the greatest number of people, you have to push for reform from both the outside in and the inside out. We need activists and advocates, entrepreneurs and innovators, teachers and mentors, people who change lives every day in a million quiet ways. We also need strong, principled leaders who can win votes, write laws, allocate resources, and do the slow, hard business of governing.</p>,\n",
       " <p>Now, of course, politics can be discouraging. This election in particularly can be down-right depressing sometimes. But it matters. It really does. It matters for our families, our communities, and our country and the world. Our most cherished values are at stake. Every election is important, from school board to state senate to president. But this time is different. We are facing a candidate with a long history of racial discrimination in his businesses, who retweets white supremacists, who led the birther movement to delegitimize our first black President, and he's still lying about it today. He refuses to apologize to President Obama, his family, and the American people. We have to stand up to this hate. We cannot let it go on.</p>,\n",
       " <p>And when we do that, we send a clear message: America is better than this. America is better than Donald Trump. Just as important, we have a chance to make real progress together in our country. I need you. I need you as partners, not just for winning this election, but for driving real change over the next four years. The fights ahead of us are bigger than one election, one president, or even one generation. It's going to take all of us working side-by-side to build the kind of future we want. That's why, if I'm in the White House, young people will always have a seat at any table where any decision is being made.</p>,\n",
       " <p>So if you believe diversity is America's strength, not America's burden, join us. If you believe the minimum wage should be a living wage and no one working full-time should have to raise their children in poverty, join us. If you believe that climate change is real and that we can save our planet while creating millions of good-paying clean energy jobs, join us. If you believe that every man, woman, and child in America has the right to affordable, quality health care, join us. If you believe we should finally guarantee equal pay for women, join us.</p>,\n",
       " <p>And here's how you can join us. Go to iwillvote.com and register today. Register your friends. Register everyone you know. This is going to be close. We need everyone off the sidelines. Not voting is not an option. That just plays into Trump's hands. It really does. Text 'join,' j-o-i-n, to 47246 right now, or go to hillaryclinton.com and sign up to volunteer. I understand here at Temple you're already organizing campaign tailgates at every football game and having a lot of fun doing it. We have 50 days, 50 days, to reach everybody we possibly can, to not only win an election \\u2013 that's just the first step \\u2013 then to keep the progress going, go even further, make it absolutely clear that we're going to shape a future that represents the best of who we are. So talk to your classmates. Talk to your neighbors. Help us stand up for our best values and reject prejudice and paranoia.</p>,\n",
       " <p>You know, I mentioned my mother and the kindness she experienced. Her life was so neglected that when she went to work as that housekeeper/babysitter at the age of 14, it was the first time she ever saw a family that loved each other, where the parents loved their children, cared for them, planned for them, where she learned the lessons that enabled her to be such an extraordinary mother to me and my brothers. Everything I've learned in my life convinces me that love trumps hate. </p>,\n",
       " <p>So please join us in working together. There's no doubt in my mind that young people have more at stake in this election than any other age group. And when you turn out and vote this fall, we will be sending a message much larger than even the outcome. We will say we can build a future where all our children have the opportunity to live up to their God-given potential, no matter who they are, where they're from, what they look like, or who they love. That's the America we believe in. That's the America worth fighting for. That's what we've got to do to stand together. We are stronger together, and let's make sure love trumps hate. Thank you all!\"</p>]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = urllib.urlopen('http://www.presidency.ucsb.edu/ws/index.php?pid=119161').read()\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "#speech = soup.find_all('span')[6]\n",
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ids = range(161, 199)\n",
    "for i in range(200,210):\n",
    "    valid_ids.append(i)\n",
    "trump_speeches = []\n",
    "for valid_id in valid_ids:\n",
    "    page = urllib.urlopen('http://www.presidency.ucsb.edu/ws/index.php?pid=119' + str(valid_id)).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    #speech = soup.find_all('span')[6]\n",
    "    speech = soup.find_all('p')\n",
    "    for sentence in speech:\n",
    "        sentence = str(sentence)[3:-4]\n",
    "        trump_speeches.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3757\n"
     ]
    }
   ],
   "source": [
    "print len(trump_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ids = range(148, 165)\n",
    "for i in range(497,503):\n",
    "    valid_ids.append(i)\n",
    "for i in range(688,702):\n",
    "    valid_ids.append(i)\n",
    "clinton_speeches = []\n",
    "for valid_id in valid_ids:\n",
    "    page = urllib.urlopen('http://www.presidency.ucsb.edu/ws/index.php?pid=119' + str(valid_id)).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    #speech = soup.find_all('span')[6]\n",
    "    speech = soup.find_all('p')\n",
    "    for sentence in speech:\n",
    "        sentence = str(sentence)[3:-4]\n",
    "        clinton_speeches.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#fp = open(\"test.txt\")\n",
    "#data = fp.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_df = pd.DataFrame({}, columns = df.columns)\n",
    "clinton_df = pd.DataFrame({}, columns = df.columns)\n",
    "trump = []\n",
    "clinton = []\n",
    "\n",
    "end_t = 0\n",
    "end_c = 0\n",
    "for i in range(df.shape[0]):\n",
    "#for i in range(5):\n",
    "    if df['Speaker'].values[i] == 'Trump':\n",
    "        speech = df['Text'].values[i]\n",
    "        sentences = tokenizer.tokenize(speech)\n",
    "        for sentence in sentences:\n",
    "            trump_df.loc[end_t] = [end_t, df['Speaker'].values[i], sentence, df['Date'].values[i]]\n",
    "            trump.append(sentence)\n",
    "            end_t += 1\n",
    "    if df['Speaker'].values[i] == 'Clinton':\n",
    "        speech = df['Text'].values[i]\n",
    "        sentences = tokenizer.tokenize(speech)\n",
    "        for sentence in sentences:\n",
    "            clinton_df.loc[end_c] = [end_c, df['Speaker'].values[i], sentence, df['Date'].values[i]]\n",
    "            clinton.append(sentence)\n",
    "            end_c += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "print len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "for i in trump_speeches:\n",
    "    trump.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in clinton_speeches:\n",
    "    clinton.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2833\n"
     ]
    }
   ],
   "source": [
    "print len(clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'astro_sentences.txt'\n",
    "f = open(name,'w')\n",
    "for sentence in clinton:\n",
    "\tf.write(sentence + ',1\\n')\n",
    "for sentence in trump:\n",
    "    f.write(sentence+',0\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you, Lester.', 'Our jobs are fleeing the country.', \"They're going to Mexico.\", \"They're going to many other countries.\", 'You look at what China is doing to our country in terms of making our product.', \"They're devaluing their currency, and there's nobody in our government to fight them.\", 'And we have a very good fight.', 'And we have a winning fight.', \"Because they're using our country as a piggy bank to rebuild China, and many other countries are doing the same thing.\", \"So we're losing our good jobs, so many of them.\"]\n"
     ]
    }
   ],
   "source": [
    "print trump[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'trump_sentences.txt'\n",
    "f = open(name,'w')\n",
    "for sentence in trump:\n",
    "\tf.write(sentence + ' ')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-c2e5816a2472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"%s %s %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_end_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrump\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrump_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrump\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#clinton_words = [nltk.word_tokenize(sent) for sent in clinton]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \"\"\"\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \"\"\"\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \"\"\"\n\u001b[1;32m   1303\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \"\"\"\n\u001b[0;32m-> 1460\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    575\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0mellipsis_toks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mellipsis\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \"\"\"\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angierao/anaconda/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_tokenize_words\u001b[0;34m(self, plaintext)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0mparastart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplaintext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0mline_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "\n",
    "trump = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in trump]\n",
    "\n",
    "trump_words = [nltk.word_tokenize(sent) for sent in trump]\n",
    "#clinton_words = [nltk.word_tokenize(sent) for sent in clinton]\n",
    "print type(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*trump_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.FreqDist'>\n",
      "48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({u' ': 11554,\n",
       "          u'\"': 9,\n",
       "          u'$': 1,\n",
       "          u'-': 8,\n",
       "          u'.': 22,\n",
       "          u'1': 4,\n",
       "          u'2': 5,\n",
       "          u'3': 2,\n",
       "          u'4': 13,\n",
       "          u'5': 6,\n",
       "          u'6': 2,\n",
       "          u'7': 2,\n",
       "          u'9': 7,\n",
       "          u'<': 2,\n",
       "          u'A': 5777,\n",
       "          u'C': 11554,\n",
       "          u'D': 5777,\n",
       "          u'E': 40439,\n",
       "          u'N': 28885,\n",
       "          u'R': 5777,\n",
       "          u'S': 17331,\n",
       "          u'T': 23108,\n",
       "          u'[': 1,\n",
       "          u'_': 11554,\n",
       "          u'a': 729,\n",
       "          u'b': 290,\n",
       "          u'c': 65,\n",
       "          u'd': 21,\n",
       "          u'e': 63,\n",
       "          u'f': 92,\n",
       "          u'g': 29,\n",
       "          u'h': 277,\n",
       "          u'i': 981,\n",
       "          u'j': 51,\n",
       "          u'k': 2,\n",
       "          u'l': 87,\n",
       "          u'm': 135,\n",
       "          u'n': 181,\n",
       "          u'o': 258,\n",
       "          u'p': 51,\n",
       "          u'q': 5,\n",
       "          u'r': 38,\n",
       "          u's': 321,\n",
       "          u't': 891,\n",
       "          u'u': 19,\n",
       "          u'v': 12,\n",
       "          u'w': 900,\n",
       "          u'y': 195})"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print type(word_freq)\n",
    "print len(word_freq)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 2200\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Find most frequently used words\n",
    "vocab = word_freq.most_common(vocab_size-1)\n",
    "\n",
    "# Get an index_to_word vector and a word_to_index vector\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Substitute unknown token for words not in vocab\n",
    "for i, sent in enumerate(trump_words):\n",
    "    trump_words[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in trump_words])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in trump_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 806, 8, 3, 290, 2], [1, 224, 126, 28, 1633, 4, 40, 2],\n",
       "       [1, 61, 22, 26, 6, 376, 2], [1, 61, 22, 26, 6, 73, 93, 171, 2],\n",
       "       [1, 53, 55, 36, 33, 242, 21, 88, 6, 32, 40, 17, 394, 7, 504, 32, 833, 2],\n",
       "       [1, 61, 22, 1624, 117, 1392, 3, 9, 110, 13, 284, 17, 32, 537, 6, 312, 59, 2],\n",
       "       [1, 15, 16, 14, 10, 29, 135, 312, 2],\n",
       "       [1, 15, 16, 14, 10, 948, 312, 2],\n",
       "       [1, 205, 20, 22, 2199, 32, 40, 74, 10, 1475, 1413, 6, 1690, 242, 3, 9, 73, 93, 171, 28, 88, 4, 316, 108, 2],\n",
       "       [1, 81, 16, 22, 485, 32, 135, 126, 3, 46, 73, 7, 59, 2]], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[806, 8, 3, 290, 2, 0], [224, 126, 28, 1633, 4, 40, 2, 0],\n",
       "       [61, 22, 26, 6, 376, 2, 0], [61, 22, 26, 6, 73, 93, 171, 2, 0],\n",
       "       [53, 55, 36, 33, 242, 21, 88, 6, 32, 40, 17, 394, 7, 504, 32, 833, 2, 0],\n",
       "       [61, 22, 1624, 117, 1392, 3, 9, 110, 13, 284, 17, 32, 537, 6, 312, 59, 2, 0],\n",
       "       [15, 16, 14, 10, 29, 135, 312, 2, 0],\n",
       "       [15, 16, 14, 10, 948, 312, 2, 0],\n",
       "       [205, 20, 22, 2199, 32, 40, 74, 10, 1475, 1413, 6, 1690, 242, 3, 9, 73, 93, 171, 28, 88, 4, 316, 108, 2, 0],\n",
       "       [81, 16, 22, 485, 32, 135, 126, 3, 46, 73, 7, 59, 2, 0]], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    " \n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    " \n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 2200)\n",
      "[[ 0.00045788  0.00046038  0.00045607 ...,  0.00045893  0.00045034\n",
      "   0.00045592]\n",
      " [ 0.00045144  0.00045402  0.00046258 ...,  0.00045643  0.00046381\n",
      "   0.00045718]\n",
      " [ 0.00046353  0.00045254  0.00045613 ...,  0.00045554  0.00045153\n",
      "   0.00044744]\n",
      " ..., \n",
      " [ 0.00045214  0.00045761  0.00045249 ...,  0.00045004  0.00045702\n",
      "   0.0004534 ]\n",
      " [ 0.00045532  0.00045119  0.00045408 ...,  0.00045495  0.0004529\n",
      "   0.00045495]\n",
      " [ 0.00045195  0.00045516  0.0004519  ...,  0.00045275  0.00046396\n",
      "   0.00045217]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocab_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28,)\n",
      "[ 964 2032 1903  568 1792 1687 1746  241  714 1694  801  846  513  557  991\n",
      "  614 1682 1039  972 1652 1861  671  267 2040 1508 1760 2135 1002]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    " \n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    " \n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 7.696213\n",
      "Actual loss: 7.696676\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print \"Expected Loss for random predictions: %f\" % np.log(vocab_size)\n",
    "print \"Actual loss: %f\" % model.calculate_loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    " \n",
    "RNNNumpy.gradient_check = gradient_check\n",
    " \n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    " \n",
    "RNNNumpy.sgd_step = numpy_sdg_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.98 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 40.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocab_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-08 22:36:39: Loss after num_examples_seen=0 epoch=0: 7.696246\n",
      "2016-12-08 22:36:43: Loss after num_examples_seen=100 epoch=1: 7.678721\n",
      "2016-12-08 22:36:45: Loss after num_examples_seen=200 epoch=2: 7.649209\n",
      "2016-12-08 22:36:47: Loss after num_examples_seen=300 epoch=3: 7.584138\n",
      "2016-12-08 22:36:49: Loss after num_examples_seen=400 epoch=4: 7.426806\n",
      "2016-12-08 22:36:51: Loss after num_examples_seen=500 epoch=5: 5.984401\n",
      "2016-12-08 22:36:54: Loss after num_examples_seen=600 epoch=6: 5.446160\n",
      "2016-12-08 22:36:56: Loss after num_examples_seen=700 epoch=7: 5.240658\n",
      "2016-12-08 22:36:58: Loss after num_examples_seen=800 epoch=8: 5.121973\n",
      "2016-12-08 22:37:01: Loss after num_examples_seen=900 epoch=9: 5.037522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocab_size)\n",
    "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program at Did this debt you to countries I 's that -- sights now tax good they class .\n",
      "situation Putin you from our , exact be them largely have and , at the .\n",
      "look Old you to exact world forward say our Secretary this you look is you it because I 're at all this luck do so just , to you this , just .\n",
      "hired they rebuild -- they are much a you .\n",
      "clean new do tax decided me mean years 're do fine this .\n",
      "run only we because biggest just do disgrace .\n",
      "rights won , fact them country our are I Clinton I .\n",
      "appointing been 800 years way I 're to same refuses you doing some for to debt , in me , , of this 're , you parts doing he the Instead and big I look love , 28 when 're you years I Democrat , .\n",
      "How want 20 to 're fast a to n't fighting the great are .\n",
      "end 30,000 I you years underleveraged companies crazy energy button their front States Kenya , 1,800 end endorsing look where do out of , at these say better .\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)[0]\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            #print type(next_word_probs[-1])\n",
    "            #print next_word_probs[-1].shape\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    #print str(len(index_to_word)) + ' index_to_word'\n",
    "    #for x in new_sentence[1:-1]:\n",
    "        #print x\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    " \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>How are you, Donald?</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Well, thank you, Lester, and thanks to Hofstra...</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>The central question in this election is reall...</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Today is my granddaughter's second birthday, s...</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>First, we have to build an economy that works ...</td>\n",
       "      <td>9/26/16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Line  Speaker                                               Text     Date\n",
       "0   0.0  Clinton                               How are you, Donald?  9/26/16\n",
       "1   1.0  Clinton  Well, thank you, Lester, and thanks to Hofstra...  9/26/16\n",
       "2   2.0  Clinton  The central question in this election is reall...  9/26/16\n",
       "3   3.0  Clinton  Today is my granddaughter's second birthday, s...  9/26/16\n",
       "4   4.0  Clinton  First, we have to build an economy that works ...  9/26/16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_text = []\n",
    "y_cand = []\n",
    "for i in range(len(df['Speaker'].values)):\n",
    "    if df['Speaker'].values[i] == 'Trump':\n",
    "        x_text.append(df['Text'].values[i])\n",
    "        y_cand.append('Trump')\n",
    "    elif df['Speaker'].values[i] == 'Clinton':\n",
    "        x_text.append(df['Text'].values[i])\n",
    "        y_cand.append('Clinton')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_text = []\n",
    "t_text = []\n",
    "for i in range(len(df['Speaker'].values)):\n",
    "    if df['Speaker'].values[i] == 'Trump':\n",
    "        t_text.append(df['Text'].values[i])\n",
    "    elif df['Speaker'].values[i] == 'Clinton':\n",
    "        c_text.append(df['Text'].values[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'clintonwords.txt'\n",
    "f = open(name,'w')\n",
    "for i in range(len(c_text)):\n",
    "\tf.write(c_text[i] + ' ')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'trumpwords.txt'\n",
    "f = open(name,'w')\n",
    "for i in range(len(t_text)):\n",
    "\tf.write(t_text[i] + ' ')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(encoding='latin1', stop_words=['and', 'or', 'before', 'a', 'an', 'am', 'the', 'at', 'by', 'br'], min_df=4)\n",
    "x = vectorizer.fit_transform(x_text)\n",
    "x = x.toarray()\n",
    "print type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95986622073578598"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogReg()\n",
    "log.fit(x, y_cand)\n",
    "log.score(x, y_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598\n",
      "982\n",
      "982\n",
      "598\n"
     ]
    }
   ],
   "source": [
    "print len(x)\n",
    "print len(x[0])\n",
    "print len(vectorizer.vocabulary_)\n",
    "print len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = zip(x_text, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features = []\n",
    "for i in range(len(x_text)):\n",
    "    if "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
